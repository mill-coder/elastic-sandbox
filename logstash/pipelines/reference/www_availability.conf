input {
  elasticsearch {
    hosts => ["${LOGSTASH_ES_HOSTS}"]
    user => "${LOGSTASH_ES_USER}"
    password => "${LOGSTASH_ES_PASSWORD}"

    # Source: heartbeat data stream
    index => "heartbeat-*"

    # Incremental import: only fetch documents newer than last run
    # IMPORTANT: Must sort by tracking_field for cursor to work correctly
    query => '{ "sort": [{ "@timestamp": "asc" }], "query": { "range": { "@timestamp": { "gt": ":last_value" } } } }'
    tracking_field => "@timestamp"
    tracking_field_seed => "1970-01-01T00:00:00.000Z"
    last_run_metadata_path => "/usr/share/logstash/data/.www_availability_last_run"

    # Use search_after for efficient pagination
    search_api => "search_after"

    # Schedule: run every 15 seconds (cron with seconds field)
    schedule => "*/15 * * * * *"
  }
}

filter {
  # Keep only required fields, remove everything else
  prune {
    whitelist_names => ["^@timestamp$", "^url$", "^monitor$", "^data_stream$"]
  }

  # Remove nested fields we don't need from url and monitor
  mutate {
    remove_field => [
      "[url][scheme]", "[url][port]", "[url][full]",
      "[monitor][ip]", "[monitor][timespan]", "[monitor][type]", "[monitor][id]",
      "[monitor][name]", "[monitor][check_group]", "[monitor][duration]"
    ]
  }

  # Set data stream coordinates for output
  mutate {
    add_field => {
      "[data_stream][type]" => "logs"
      "[data_stream][dataset]" => "www.availability"
      "[data_stream][namespace]" => "analytics"
    }
  }
}

output {
  elasticsearch {
    hosts => ["${LOGSTASH_ES_HOSTS}"]
    user => "${LOGSTASH_ES_USER}"
    password => "${LOGSTASH_ES_PASSWORD}"
    data_stream => true
  }
}
